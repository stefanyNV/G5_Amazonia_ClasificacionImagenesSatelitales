{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazonia5_TransferLearning_VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tgXpOHtwlRxB",
        "oZE3ziBymGFZ",
        "FU2xsY18tZ2A"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_L6zMPYJtGg"
      },
      "source": [
        "# **Indagando los pasos del VGG16**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vDKHwBveyGz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Npr5FLdns_"
      },
      "source": [
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF8h49DBlNZE"
      },
      "source": [
        "## Train y test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp3EZIgMi8FL",
        "outputId": "d9dc260d-8984-406a-c962-8c7b78e4ff15"
      },
      "source": [
        "# load dataset\n",
        "data = load('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/planet_data_128_2.npz')\n",
        "X, y = data['arr_0'], data['arr_1']\n",
        "print('Loaded: ', X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded:  (1440, 128, 128, 3) (1440, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1RK0N0MjpqW",
        "outputId": "4b0bd36c-2d62-4ac1-b9eb-f004089abe5e"
      },
      "source": [
        "# separate into train and test datasets\n",
        "trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "print(trainX.shape, trainY.shape, testX.shape, testY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1008, 128, 128, 3) (1008, 17) (432, 128, 128, 3) (432, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgXpOHtwlRxB"
      },
      "source": [
        "# Fbeta score para multi-class/label classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL1AICY8lg8n"
      },
      "source": [
        "# calculate fbeta score for multi-class/label classification\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "\t# clip predictions\n",
        "\ty_pred = backend.clip(y_pred, 0, 1)\n",
        "\t# calculate elements\n",
        "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\t# calculate precision\n",
        "\tp = tp / (tp + fp + backend.epsilon())\n",
        "\t# calculate recall\n",
        "\tr = tp / (tp + fn + backend.epsilon())\n",
        "\t# calculate fbeta, averaged across each class\n",
        "\tbb = beta ** 2\n",
        "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
        "\treturn fbeta_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZE3ziBymGFZ"
      },
      "source": [
        "# Modelo CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvBHTPfLmP2b"
      },
      "source": [
        "# parametros\n",
        "in_shape=(128, 128, 3)\n",
        "out_shape=17"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHUqNAVMmHWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003168b8-c016-47bc-9bdd-8e4887fc541f"
      },
      "source": [
        "# load model\n",
        "model = VGG16(include_top=False, input_shape=in_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_omuvNXYmYE0"
      },
      "source": [
        "# mark loaded layers as not trainable\n",
        "for layer in model.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtSAlVFxmjiz"
      },
      "source": [
        "# allow last vgg block to be trainable\n",
        "model.get_layer('block5_conv1').trainable = True\n",
        "model.get_layer('block5_conv2').trainable = True\n",
        "model.get_layer('block5_conv3').trainable = True\n",
        "model.get_layer('block5_pool').trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV6AYWcYmmx6"
      },
      "source": [
        "# add new classifier layers\n",
        "flat1 = Flatten()(model.layers[-1].output)\n",
        "class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "output = Dense(out_shape, activation='sigmoid')(class1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQM07c1YmqEm"
      },
      "source": [
        "# define new model\n",
        "model = Model(inputs=model.inputs, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N9FW5ybmtRo"
      },
      "source": [
        "# compile model\n",
        "opt = SGD(lr=0.01, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxvYbqOHtDTO",
        "outputId": "665ea2b5-275f-4eca-d0b1-9bde5e3075b1"
      },
      "source": [
        "model.summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Model.summary of <tensorflow.python.keras.engine.functional.Functional object at 0x7f5c500bae80>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSjQo9QOmJ7R"
      },
      "source": [
        "#return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU2xsY18tZ2A"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_SspGxmtw25"
      },
      "source": [
        "#input -> history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW1FberbtAPk"
      },
      "source": [
        "# plot loss\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Cross Entropy Loss')\n",
        "pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "pyplot.plot(history.history['val_loss'], color='orange', label='test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw0MURMJtmcS"
      },
      "source": [
        "# plot accuracy\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Fbeta')\n",
        "pyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
        "pyplot.plot(history.history['val_fbeta'], color='orange', label='test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx9SFeJMtqcT"
      },
      "source": [
        "# save plot to file\n",
        "filename = sys.argv[0].split('/')[-1]\n",
        "pyplot.savefig(filename + '_plot.png')\n",
        "pyplot.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvmG6v2mt-6w"
      },
      "source": [
        "# Evaluaci√≥n del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tyKtFrjv1n9",
        "outputId": "2e28768d-da3e-43f0-eca1-ecbe7017f9f3"
      },
      "source": [
        "# load dataset\n",
        "data = load('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/planet_data_128_2.npz')\n",
        "X, y = data['arr_0'], data['arr_1']\n",
        "print('Loaded: ', X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded:  (1440, 128, 128, 3) (1440, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu39psPEv1n-",
        "outputId": "c8548ca2-da91-4e4a-d03e-ba9921f5efb7"
      },
      "source": [
        "trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "print(trainX.shape, trainY.shape, testX.shape, testY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1008, 128, 128, 3) (1008, 17) (432, 128, 128, 3) (432, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAU8XGPjuZXr"
      },
      "source": [
        "# create data generator\n",
        "train_datagen = ImageDataGenerator(featurewise_center=True, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
        "test_datagen = ImageDataGenerator(featurewise_center=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quEqBFvYueKx"
      },
      "source": [
        "# specify imagenet mean values for centering\n",
        "train_datagen.mean = [123.68, 116.779, 103.939]\n",
        "test_datagen.mean = [123.68, 116.779, 103.939]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQCGyR1Duhck"
      },
      "source": [
        "# prepare iterators\n",
        "train_it = train_datagen.flow(trainX, trainY, batch_size=128)\n",
        "test_it = test_datagen.flow(testX, testY, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0weVQsDZukKP"
      },
      "source": [
        "# define model\n",
        "model = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ekTQddRunMy",
        "outputId": "021aa128-b901-4c09-8814-72c439d25f33"
      },
      "source": [
        "# fit model\n",
        "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
        "validation_data = test_it, validation_steps=len(test_it), epochs=50, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-46-a77d66a7ebc0>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meo3_yya21Lh"
      },
      "source": [
        "# save model\n",
        "model.save('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/model_vgg16.h5')\n",
        "\n",
        "# save both \n",
        "savez_compressed('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/history_vgg16.npz', history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gs69auRusRJ"
      },
      "source": [
        "# evaluate model\n",
        "loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
        "print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTicFO3wuvYN"
      },
      "source": [
        "# learning curves\n",
        "summarize_diagnostics(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz0dIPQLe8sQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlbcuhPEenSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa102c6c-7096-4f38-e7ad-a8502b57cc2b"
      },
      "source": [
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/planet_data_128_2.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# separate into train and test datasets\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "\t# clip predictions\n",
        "\ty_pred = backend.clip(y_pred, 0, 1)\n",
        "\t# calculate elements\n",
        "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\t# calculate precision\n",
        "\tp = tp / (tp + fp + backend.epsilon())\n",
        "\t# calculate recall\n",
        "\tr = tp / (tp + fn + backend.epsilon())\n",
        "\t# calculate fbeta, averaged across each class\n",
        "\tbb = beta ** 2\n",
        "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
        "\treturn fbeta_score\n",
        "\n",
        "def define_model(in_shape=(128, 128, 3), out_shape=17):\n",
        "\t# load model\n",
        "\tmodel = VGG16(include_top=False, input_shape=in_shape)\n",
        "\t# mark loaded layers as not trainable\n",
        "\tfor layer in model.layers:\n",
        "\t\tlayer.trainable = False\n",
        "\t# allow last vgg block to be trainable\n",
        "\tmodel.get_layer('block5_conv1').trainable = True\n",
        "\tmodel.get_layer('block5_conv2').trainable = True\n",
        "\tmodel.get_layer('block5_conv3').trainable = True\n",
        "\tmodel.get_layer('block5_pool').trainable = True\n",
        "\t# add new classifier layers\n",
        "\tflat1 = Flatten()(model.layers[-1].output)\n",
        "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "\toutput = Dense(out_shape, activation='sigmoid')(class1)\n",
        "\t# define new model\n",
        "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.01, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "\treturn model\n",
        "\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Fbeta')\n",
        "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\t#filename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/m128_2_vgg16_plot.png')\n",
        "\tpyplot.close()\n",
        "\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\t# create data generator\n",
        "\ttrain_datagen = ImageDataGenerator(featurewise_center=True, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
        "\ttest_datagen = ImageDataGenerator(featurewise_center=True)\n",
        "\t# specify imagenet mean values for centering\n",
        "\ttrain_datagen.mean = [123.68, 116.779, 103.939]\n",
        "\ttest_datagen.mean = [123.68, 116.779, 103.939]\n",
        "\t# prepare iterators\n",
        "\ttrain_it = train_datagen.flow(trainX, trainY, batch_size=128)\n",
        "\ttest_it = test_datagen.flow(testX, testY, batch_size=128)\n",
        "\t# define model\n",
        "\tmodel = define_model()\n",
        "\t# fit model\n",
        "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0)\n",
        "\t# evaluate model\n",
        "\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1008, 128, 128, 3) (1008, 17) (432, 128, 128, 3) (432, 17)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From <ipython-input-3-aa3554944c3c>:98: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "WARNING:tensorflow:From <ipython-input-3-aa3554944c3c>:100: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.evaluate, which supports generators.\n",
            "> loss=0.364, fbeta=0.652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o08gkKVjKUZK"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld0qj7vzI9Kb"
      },
      "source": [
        "model.save('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/modelo128_2_vgg16_gpu.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-TQ523SMEM6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBG_ad1OMDtI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdkFj7i0MDUy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVmYfCE_MFLJ",
        "outputId": "8226f61f-6c8e-49ce-a2dd-93b4666904f3"
      },
      "source": [
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def load_dataset():\n",
        "\tdata = load('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/planet_data_128_2.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "\ty_pred = backend.clip(y_pred, 0, 1)\n",
        "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\tp = tp / (tp + fp + backend.epsilon())\n",
        "\tr = tp / (tp + fn + backend.epsilon())\n",
        "\tbb = beta ** 2\n",
        "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
        "\treturn fbeta_score\n",
        "\n",
        "def define_model(in_shape=(128, 128, 3), out_shape=17):\n",
        "\tmodel = VGG16(include_top=False, input_shape=in_shape)\n",
        "\tfor layer in model.layers:\n",
        "\t\tlayer.trainable = False\n",
        "\tmodel.get_layer('block5_conv1').trainable = True\n",
        "\tmodel.get_layer('block5_conv2').trainable = True\n",
        "\tmodel.get_layer('block5_conv3').trainable = True\n",
        "\tmodel.get_layer('block5_pool').trainable = True\n",
        "\tflat1 = Flatten()(model.layers[-1].output)\n",
        "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "\toutput = Dense(out_shape, activation='sigmoid')(class1)\n",
        "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
        "\topt = SGD(lr=0.01, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "\treturn model\n",
        "\n",
        "def summarize_diagnostics(history):\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Fbeta')\n",
        "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
        "\n",
        "\tpyplot.savefig('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/m128_2_vgg16_plot.png')\n",
        "\tpyplot.close()\n",
        "\n",
        "def run_test_harness():\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\ttrain_datagen = ImageDataGenerator(featurewise_center=True, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
        "\ttest_datagen = ImageDataGenerator(featurewise_center=True)\n",
        "\ttrain_datagen.mean = [123.68, 116.779, 103.939]\n",
        "\ttest_datagen.mean = [123.68, 116.779, 103.939]\n",
        "\n",
        "\ttrain_it = train_datagen.flow(trainX, trainY, batch_size=128)\n",
        "\ttest_it = test_datagen.flow(testX, testY, batch_size=128)\n",
        "\n",
        "\tmodel = define_model()\n",
        "\n",
        "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0)\n",
        "\n",
        "\tmodel.save('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/modelo128_2_vgg16_gpu.h5')\n",
        "\n",
        "\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "\n",
        "\tsummarize_diagnostics(history)\n",
        "\n",
        "run_test_harness()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1008, 128, 128, 3) (1008, 17) (432, 128, 128, 3) (432, 17)\n",
            "> loss=0.151, fbeta=0.856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdGCnEE_PPxC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5EJWDR8PPoc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7YErkRbPPd6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pViRTnuPPRp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-ynU7M_PQRE",
        "outputId": "470bd501-755a-468d-e608-74ff2671cb2b"
      },
      "source": [
        "# vgg with fine-tuning and data augmentation for the planet dataset\n",
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/planet_data_64_all_train.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# separate into train and test datasets\n",
        "\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n",
        "# calculate fbeta score for multi-class/label classification\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "\t# clip predictions\n",
        "\ty_pred = backend.clip(y_pred, 0, 1)\n",
        "\t# calculate elements\n",
        "\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\t# calculate precision\n",
        "\tp = tp / (tp + fp + backend.epsilon())\n",
        "\t# calculate recall\n",
        "\tr = tp / (tp + fn + backend.epsilon())\n",
        "\t# calculate fbeta, averaged across each class\n",
        "\tbb = beta ** 2\n",
        "\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
        "\treturn fbeta_score\n",
        "\n",
        "# define cnn model\n",
        "def define_model(in_shape=(64, 64, 3), out_shape=17):\n",
        "\t# load model\n",
        "\tmodel = VGG16(include_top=False, input_shape=in_shape)\n",
        "\t# mark loaded layers as not trainable\n",
        "\tfor layer in model.layers:\n",
        "\t\tlayer.trainable = False\n",
        "\t# allow last vgg block to be trainable\n",
        "\tmodel.get_layer('block5_conv1').trainable = True\n",
        "\tmodel.get_layer('block5_conv2').trainable = True\n",
        "\tmodel.get_layer('block5_conv3').trainable = True\n",
        "\tmodel.get_layer('block5_pool').trainable = True\n",
        "\t# add new classifier layers\n",
        "\tflat1 = Flatten()(model.layers[-1].output)\n",
        "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "\toutput = Dense(out_shape, activation='sigmoid')(class1)\n",
        "\t# define new model\n",
        "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.01, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "\treturn model\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Fbeta')\n",
        "\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\t#filename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/m64_all_train_vgg16_plot.png')\n",
        "\tpyplot.close()\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\t# create data generator\n",
        "\ttrain_datagen = ImageDataGenerator(featurewise_center=True, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
        "\ttest_datagen = ImageDataGenerator(featurewise_center=True)\n",
        "\t# specify imagenet mean values for centering\n",
        "\ttrain_datagen.mean = [123.68, 116.779, 103.939]\n",
        "\ttest_datagen.mean = [123.68, 116.779, 103.939]\n",
        "\t# prepare iterators\n",
        "\ttrain_it = train_datagen.flow(trainX, trainY, batch_size=128)\n",
        "\ttest_it = test_datagen.flow(testX, testY, batch_size=128)\n",
        "\t# define model\n",
        "\tmodel = define_model()\n",
        "\t# fit model\n",
        "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0)\n",
        "\t# save model\n",
        "\tmodel.save('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/modelo64_all_train_vgg16_gpu.h5')\n",
        "\t# evaluate model\n",
        "\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28327, 64, 64, 3) (28327, 17) (12141, 64, 64, 3) (12141, 17)\n",
            "WARNING:tensorflow:From <ipython-input-2-05ab2ccbfad8>:102: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.evaluate, which supports generators.\n",
            "> loss=0.124, fbeta=0.873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li1zN_p7djL9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iYLcFNPdixN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TpvNpNjdinS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38bS8gaTdidU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbDF6DcediUA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elsBV-f_dtP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881683df-98e7-4b79-b549-90b1e7a7fe57"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfG5cNvUXvPr"
      },
      "source": [
        "# vgg with fine-tuning and data augmentation for the planet dataset\n",
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/planet_data_64_all_train2.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# # separate into train and test datasets\n",
        "\t# trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\t# print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn X, y\n",
        "\n",
        "# define cnn model\n",
        "def define_model(in_shape=(64, 64, 3), out_shape=17):\n",
        "\t# load model\n",
        "\tmodel = VGG16(include_top=False, input_shape=in_shape)\n",
        "\t# mark loaded layers as not trainable\n",
        "\tfor layer in model.layers:\n",
        "\t\tlayer.trainable = False\n",
        "\t# allow last vgg block to be trainable\n",
        "\tmodel.get_layer('block5_conv1').trainable = True\n",
        "\tmodel.get_layer('block5_conv2').trainable = True\n",
        "\tmodel.get_layer('block5_conv3').trainable = True\n",
        "\tmodel.get_layer('block5_pool').trainable = True\n",
        "\t# add new classifier layers\n",
        "\tflat1 = Flatten()(model.layers[-1].output)\n",
        "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "\toutput = Dense(out_shape, activation='sigmoid')(class1)\n",
        "\t# define new model\n",
        "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.01, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "\treturn model\n",
        "\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\tX, y = load_dataset()\n",
        "\t# create data generator\n",
        "\ttrain_datagen = ImageDataGenerator(featurewise_center=True, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
        "\t# specify imagenet mean values for centering\n",
        "\ttrain_datagen.mean = [123.68, 116.779, 103.939]\n",
        "\t# prepare iterators\n",
        "\ttrain_it = train_datagen.flow(X, y, batch_size=128)\n",
        "\t# define model\n",
        "\tmodel = define_model()\n",
        "\t# fit model\n",
        "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it), epochs=50, verbose=0)\n",
        " \t# save model\n",
        "\tmodel.save('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/modelo64_all_train_vgg16_gpu_final2.h5')\n",
        "\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFuwcsddShDv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm1g_h7LhN94"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyZzEIj9hN4y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SFqTIwrhNxM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evkj6HwRhNl8",
        "outputId": "43fdd695-518d-46a9-eafe-f1e7384e9484"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFxpVMZ6Sheu",
        "outputId": "21675be1-f241-45f4-f635-53f6a19dee94"
      },
      "source": [
        "# vgg with fine-tuning and data augmentation for the planet dataset\n",
        "import sys\n",
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\tdata = load('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/planet_data_64_all_train2.npz')\n",
        "\tX, y = data['arr_0'], data['arr_1']\n",
        "\t# # separate into train and test datasets\n",
        "\t# trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\t# print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "\treturn X, y\n",
        "\n",
        "# define cnn model\n",
        "def define_model(in_shape=(64, 64, 3), out_shape=17):\n",
        "\t# load model\n",
        "\tmodel = VGG16(include_top=False, input_shape=in_shape)\n",
        "\t# mark loaded layers as not trainable\n",
        "\tfor layer in model.layers:\n",
        "\t\tlayer.trainable = False\n",
        "\t# allow last vgg block to be trainable\n",
        "\tmodel.get_layer('block5_conv1').trainable = True\n",
        "\tmodel.get_layer('block5_conv2').trainable = True\n",
        "\tmodel.get_layer('block5_conv3').trainable = True\n",
        "\tmodel.get_layer('block5_pool').trainable = True\n",
        "\t# add new classifier layers\n",
        "\tflat1 = Flatten()(model.layers[-1].output)\n",
        "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "\toutput = Dense(out_shape, activation='sigmoid')(class1)\n",
        "\t# define new model\n",
        "\tmodel = Model(inputs=model.inputs, outputs=output)\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.01, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "\treturn model\n",
        "\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\tX, y = load_dataset()\n",
        "\t# create data generator\n",
        "\ttrain_datagen = ImageDataGenerator(featurewise_center=True, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
        "\t# specify imagenet mean values for centering\n",
        "\ttrain_datagen.mean = [123.68, 116.779, 103.939]\n",
        "\t# prepare iterators\n",
        "\ttrain_it = train_datagen.flow(X, y, batch_size=128)\n",
        "\t# define model\n",
        "\tmodel = define_model()\n",
        "\t# fit model\n",
        "\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it), epochs=50, verbose=0)\n",
        " \t# save model\n",
        "\tmodel.save('/content/drive/MyDrive/Colab Notebooks/DiplomadoIA/M1_AplicacionesConVisionArtificial/Proyecto_Amazonia/Datos_Entrenamiento/modelo64_all_train_vgg16_gpu_final2.h5')\n",
        "\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-b0d5e62bc098>:61: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}